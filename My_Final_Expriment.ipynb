{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMhwTmLQphz4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os, re, time, math, tqdm, itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.offline as pyo\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import keras\n",
        "from keras.layers import Conv2D, Conv1D, MaxPooling2D, MaxPooling1D, Flatten, BatchNormalization, Dense\n",
        "#from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "i0QekT4Dputs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Dataset/IDS 2018 Intrusion CSVs (CSE-CIC-IDS2018)/Expremente-Dataset/Dataset to Expreminte1.csv')"
      ],
      "metadata": {
        "id": "wZ34FKoupyfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(['Timestamp'], axis=1)"
      ],
      "metadata": {
        "id": "VFyuuJdrp6y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"Label\"].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "To7EOfdlp8ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the features for correlation analysis\n",
        "features = ['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts',\n",
        "            'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max',\n",
        "            'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std',\n",
        "            'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean',\n",
        "            'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean',\n",
        "            'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot',\n",
        "            'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min',\n",
        "            'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n",
        "            'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags',\n",
        "            'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s',\n",
        "            'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean',\n",
        "            'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt',\n",
        "            'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt',\n",
        "            'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg',\n",
        "            'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg',\n",
        "            'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg',\n",
        "            'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts',\n",
        "            'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts',\n",
        "            'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts',\n",
        "            'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max',\n",
        "            'Active Min', 'Idle Mean', 'Idle Std','Idle Max','Idle Min','Label']\n",
        "\n",
        "# Extract the relevant columns from the dataset\n",
        "subset = data[features]\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = subset.corr()\n",
        "\n",
        "# Display the correlation matrix\n",
        "print(correlation_matrix)"
      ],
      "metadata": {
        "id": "VPJKi7pDQhoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Select the features for correlation analysis\n",
        "features = ['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts',\n",
        "            'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max',\n",
        "            'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std',\n",
        "            'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean',\n",
        "            'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean',\n",
        "            'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot',\n",
        "            'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min',\n",
        "            'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n",
        "            'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags',\n",
        "            'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s',\n",
        "            'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean',\n",
        "            'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt',\n",
        "            'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt',\n",
        "            'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg',\n",
        "            'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg',\n",
        "            'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg',\n",
        "            'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts',\n",
        "            'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts',\n",
        "            'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts',\n",
        "            'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max',\n",
        "            'Active Min', 'Idle Mean', 'Idle Std','Idle Max','Idle Min','Label' ]\n",
        "\n",
        "# Extract the relevant columns from the dataset\n",
        "subset = data[features]\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = subset.corr()\n",
        "\n",
        "# Plot the correlation matrix\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.heatmap(correlation_matrix, vmin=-1, vmax=1, cmap='coolwarm', annot=True)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "D1Pg3fgMZHHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "# drop na values and reset index\n",
        "data_clean = data.dropna().reset_index()\n",
        "\n",
        "# label encoding\n",
        "labelencoder = LabelEncoder()\n",
        "data_clean['Label'] = labelencoder.fit_transform(data_clean['Label'])\n",
        "\n",
        "data_np = data_clean.to_numpy(dtype=\"float32\")\n",
        "# drop inf values\n",
        "data_np = data_np[~np.isinf(data_np).any(axis=1)]\n",
        "\n",
        "X = data_np[:, 0:78]\n",
        "\n",
        "enc = OneHotEncoder()\n",
        "\n",
        "Y = enc.fit_transform(data_np[:, 80:]).toarray()\n",
        "\n",
        "# Scale data to have mean 0 and variance 1\n",
        "# which is important for convergence of the neural network\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Outlier detection using Local Outlier Factor (LOF)\n",
        "lof = LocalOutlierFactor(contamination=0.05)  # contamination: expected proportion of outliers\n",
        "outliers = lof.fit_predict(X_scaled) == -1\n",
        "\n",
        "# Remove outliers from the data\n",
        "X_cleaned = X_scaled[~outliers]\n",
        "Y_cleaned = Y[~outliers]\n",
        "\n",
        "# Split the data set into training and testing\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X_cleaned, Y_cleaned, test_size=0.20, random_state=2, shuffle=True)\n",
        "\n",
        "_features = X.shape[1]\n",
        "n_classes = Y.shape[1]\n"
      ],
      "metadata": {
        "id": "2s9oRC26s3_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ],
      "metadata": {
        "id": "Q-S1pNZZgG9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to ignore FutureWarning\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras import callbacks\n",
        "from keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout\n",
        "from sklearn import metrics\n",
        "from hyperopt import fmin, hp, tpe, Trials, STATUS_OK\n",
        "from hyperopt.plotting import main_plot_history, main_plot_vars\n",
        "import uuid\n",
        "import gc\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "rt0gfwu6TscC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "metadata": {
        "id": "2P7sAtkv7AZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=64, kernel_size=5, activation='relu',\n",
        "                    padding='same', input_shape=(78, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    # adding a pooling layer\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "\n",
        "\n",
        "    model.add(Conv1D(filters=64, kernel_size=5, activation='relu',\n",
        "                    padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "\n",
        "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu',\n",
        "                    padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "\n",
        "    model.add(Conv1D(filters=256, kernel_size=5, activation='relu',\n",
        "                    padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(15, activation='softmax'))\n",
        "\n",
        "    optimizer = tf.keras.optimizers.RMSprop(lr=0.0001)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "V0iCO2V9Tt3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "7BYXVGBATwse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger = CSVLogger('logs.csv', append=True)\n",
        "his = model.fit(X_train, Y_train, epochs=50, batch_size=64,\n",
        "          validation_data=(X_test, Y_test), callbacks=[logger])"
      ],
      "metadata": {
        "id": "RwvJlhIM2twa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(X_test, Y_test)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))"
      ],
      "metadata": {
        "id": "y0rn2v7N8nWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import datetime as dt\n",
        "\n",
        "def confusion_matrix_func(y_test, y_test_pred):\n",
        "\n",
        "    '''\n",
        "    This function computes the confusion matrix using Predicted and Actual values and plots a confusion matrix heatmap\n",
        "    '''\n",
        "    C = confusion_matrix(y_test, y_test_pred)\n",
        "    cm_df = pd.DataFrame(C)\n",
        "    labels = ['Benign','Bot', 'Brute Force -Web', 'Brute Force -XSS', 'DDOS attack-HOIC', 'DDOS attack-LOIC-UDP',\n",
        "              'DDoS attacks-LOIC-HTTP', 'DoS attacks-GoldenEye', 'DoS attacks-Hulk', 'DoS attacks-SlowHTTPTest', 'DoS attacks-Slowloris', 'FTP-BruteForce', 'Infilteration', 'SQL Injection', 'SSH-Bruteforce']\n",
        "    plt.figure(figsize=(20,15))\n",
        "    sns.set(font_scale=1.4)\n",
        "    sns.heatmap(cm_df, annot=True, annot_kws={\"size\":12}, fmt='g', xticklabels=labels, yticklabels=labels)\n",
        "    plt.ylabel('Actual Class')\n",
        "    plt.xlabel('Predicted Class')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# calculate roc curve\n",
        "from sklearn.metrics import *\n",
        "#fpr_RF, tpr_RF, thresholds_RF = roc_curve(y_test, pred)\n",
        "from sklearn import preprocessing\n",
        "def multiclass_roc_auc_score(y_test, pred, average=\"macro\"):\n",
        "    lb = preprocessing.LabelBinarizer()\n",
        "    lb.fit(y_test)\n",
        "    y_test = lb.transform(y_test)\n",
        "    pred = lb.transform(pred)\n",
        "    return roc_auc_score(y_test, pred, average=average)"
      ],
      "metadata": {
        "id": "ytJIXz5k8sLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train data')\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print('='*20)\n",
        "print('Test data')\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)\n",
        "print('='*20)\n",
        "\n",
        "# Measure accuracy\n",
        "import datetime as dt\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
        "print('Predicting on the test data:')\n",
        "start = dt.datetime.now()\n",
        "escore = model.evaluate(X_test, Y_test, batch_size=128)\n",
        "pred = model.predict(X_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "y_eval = np.argmax(Y_test,axis=1)\n",
        "\n",
        "vscore = metrics.accuracy_score(y_eval, pred)\n",
        "\n",
        "rscore = recall_score(y_eval, pred, average='weighted')\n",
        "\n",
        "ascore = precision_score(y_eval, pred, average='weighted')\n",
        "\n",
        "f1score= f1_score(y_eval, pred, average='weighted') #F1 = 2 * (precision * recall) / (precision + recall) for manual\n",
        "\n",
        "accuracy = multiclass_roc_auc_score(y_eval, pred)\n",
        "\n",
        "\n",
        "print('Completed')\n",
        "print('Time taken:',dt.datetime.now()-start)\n",
        "print('='*50)\n",
        "print(\"Validation score: {}\".format(vscore))\n",
        "print('='*50)\n",
        "print(\"Evaluation score: {}\".format(escore))\n",
        "print('='*50)\n",
        "print(\"Recall score: {}\".format(rscore))\n",
        "print('='*50)\n",
        "print(\"Precision score: {}\".format(ascore))\n",
        "print('='*50)\n",
        "print(\"F1 score: {}\".format(f1score))\n",
        "print('='*50)\n"
      ],
      "metadata": {
        "id": "V-ab1Y1U8vGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = his.history\n",
        "history.keys()"
      ],
      "metadata": {
        "id": "Bvzk9bEs89Gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(history['loss']) + 1)\n",
        "acc = history['accuracy']\n",
        "loss = history['loss']\n",
        "val_acc = history['val_accuracy']\n",
        "val_loss = history['val_loss']\n",
        "\n",
        "# visualize training and val accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Accuracy (CNN)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(epochs, val_acc, label='Validation_accuracy')\n",
        "plt.plot(epochs, acc, label='Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# visualize train and val loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Loss(CNN)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(epochs, val_loss, label='Validation_loss', color='r')\n",
        "plt.plot(epochs, loss, label='Loss', color='g')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "5NUnSj0i9BBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix_func(y_eval, pred)"
      ],
      "metadata": {
        "id": "GppJ-jxF9Q6Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMhwTmLQphz4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import numpy as np  # linear algebra\n",
        "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os, re, time, math, tqdm, itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.offline as pyo\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Conv1D, MaxPooling2D, MaxPooling1D, Flatten, BatchNormalization, Dense, Dropout\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from hyperopt import fmin, hp, tpe, Trials, STATUS_OK\n",
        "from hyperopt.plotting import main_plot_history, main_plot_vars\n",
        "import uuid\n",
        "import gc\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
        "import datetime as dt"
      ],
      "metadata": {
        "id": "i0QekT4Dputs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Dataset/IDS 2018 Intrusion CSVs (CSE-CIC-IDS2018)/Expremente-Dataset/Dataset to Expreminte1.csv')"
      ],
      "metadata": {
        "id": "wZ34FKoupyfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(['Timestamp'], axis=1)"
      ],
      "metadata": {
        "id": "VFyuuJdrp6y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"Label\"].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "To7EOfdlp8ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "c41RvBiguKRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the features for correlation analysis\n",
        "features = ['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts',\n",
        "            'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max',\n",
        "            'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std',\n",
        "            'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean',\n",
        "            'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean',\n",
        "            'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot',\n",
        "            'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min',\n",
        "            'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n",
        "            'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags',\n",
        "            'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s',\n",
        "            'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean',\n",
        "            'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt',\n",
        "            'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt',\n",
        "            'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg',\n",
        "            'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg',\n",
        "            'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg',\n",
        "            'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts',\n",
        "            'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts',\n",
        "            'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts',\n",
        "            'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max',\n",
        "            'Active Min', 'Idle Mean', 'Idle Std','Idle Max','Idle Min','Label']\n",
        "\n",
        "# Exclude non-numeric columns from the subset\n",
        "numeric_subset = subset.select_dtypes(include=['number'])\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = numeric_subset.corr()\n",
        "\n",
        "# Display the correlation matrix\n",
        "print(correlation_matrix)\n"
      ],
      "metadata": {
        "id": "VPJKi7pDQhoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the features for correlation analysis\n",
        "features = ['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts',\n",
        "            'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max',\n",
        "            'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std',\n",
        "            'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean',\n",
        "            'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean',\n",
        "            'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot',\n",
        "            'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min',\n",
        "            'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n",
        "            'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags',\n",
        "            'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s',\n",
        "            'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean',\n",
        "            'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt',\n",
        "            'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt',\n",
        "            'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg',\n",
        "            'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg',\n",
        "            'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg',\n",
        "            'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts',\n",
        "            'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts',\n",
        "            'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts',\n",
        "            'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max',\n",
        "            'Active Min', 'Idle Mean', 'Idle Std','Idle Max','Idle Min','Label' ]\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = numeric_subset.corr()\n",
        "\n",
        "# Plot the correlation matrix\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.heatmap(correlation_matrix, vmin=-1, vmax=1, cmap='coolwarm', annot=True)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D1Pg3fgMZHHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop na values and reset index\n",
        "data_clean = data.dropna().reset_index()\n",
        "\n",
        "# label encoding\n",
        "labelencoder = LabelEncoder()\n",
        "data_clean['Label'] = labelencoder.fit_transform(data_clean['Label'])\n",
        "\n",
        "data_np = data_clean.to_numpy(dtype=\"float32\")\n",
        "# drop inf values\n",
        "data_np = data_np[~np.isinf(data_np).any(axis=1)]\n",
        "\n",
        "X = data_np[:, 0:78]\n",
        "\n",
        "enc = OneHotEncoder()\n",
        "\n",
        "Y = enc.fit_transform(data_np[:, 80:]).toarray()\n",
        "\n",
        "# Scale data to have mean 0 and variance 1\n",
        "# which is important for convergence of the neural network\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Outlier detection using Local Outlier Factor (LOF)\n",
        "lof = LocalOutlierFactor(contamination=0.05)  # contamination: expected proportion of outliers\n",
        "outliers = lof.fit_predict(X_scaled) == -1\n",
        "\n",
        "# Remove outliers from the data\n",
        "X_cleaned = X_scaled[~outliers]\n",
        "Y_cleaned = Y[~outliers]\n",
        "\n",
        "# Split the data set into training and testing\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X_cleaned, Y_cleaned, test_size=0.20, random_state=2, shuffle=True)\n",
        "\n",
        "_features = X.shape[1]\n",
        "n_classes = Y.shape[1]\n"
      ],
      "metadata": {
        "id": "2s9oRC26s3_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ],
      "metadata": {
        "id": "Q-S1pNZZgG9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=64, kernel_size=5, activation='relu', padding='same', input_shape=(78, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(MaxPooling1D(pool_size=3, strides=2, padding='same'))\n",
        "\n",
        "    model.add(Conv1D(filters=64, kernel_size=5, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(MaxPooling1D(pool_size=3, strides=2, padding='same'))\n",
        "\n",
        "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(MaxPooling1D(pool_size=3, strides=2, padding='same'))\n",
        "\n",
        "    model.add(Conv1D(filters=256, kernel_size=5, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(MaxPooling1D(pool_size=3, strides=2, padding='same'))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(15, activation='softmax'))\n",
        "\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "V0iCO2V9Tt3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "7BYXVGBATwse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger = CSVLogger('logs.csv', append=True)\n",
        "his = model.fit(X_train, Y_train, epochs=10, batch_size=64,\n",
        "          validation_data=(X_test, Y_test), callbacks=[logger])"
      ],
      "metadata": {
        "id": "RwvJlhIM2twa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(X_test, Y_test)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))"
      ],
      "metadata": {
        "id": "y0rn2v7N8nWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def confusion_matrix_func(y_test, y_test_pred):\n",
        "\n",
        "    '''\n",
        "    This function computes the confusion matrix using Predicted and Actual values and plots a confusion matrix heatmap\n",
        "    '''\n",
        "    C = confusion_matrix(y_test, y_test_pred)\n",
        "    cm_df = pd.DataFrame(C)\n",
        "    labels = ['Benign','Bot', 'Brute Force -Web', 'Brute Force -XSS', 'DDOS attack-HOIC', 'DDOS attack-LOIC-UDP',\n",
        "              'DDoS attacks-LOIC-HTTP', 'DoS attacks-GoldenEye', 'DoS attacks-Hulk', 'DoS attacks-SlowHTTPTest', 'DoS attacks-Slowloris', 'FTP-BruteForce', 'Infilteration', 'SQL Injection', 'SSH-Bruteforce']\n",
        "    plt.figure(figsize=(20,15))\n",
        "    sns.set(font_scale=1.4)\n",
        "    sns.heatmap(cm_df, annot=True, annot_kws={\"size\":12}, fmt='g', xticklabels=labels, yticklabels=labels)\n",
        "    plt.ylabel('Actual Class')\n",
        "    plt.xlabel('Predicted Class')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# calculate roc curve\n",
        "from sklearn.metrics import *\n",
        "#fpr_RF, tpr_RF, thresholds_RF = roc_curve(y_test, pred)\n",
        "from sklearn import preprocessing\n",
        "def multiclass_roc_auc_score(y_test, pred, average=\"macro\"):\n",
        "    lb = preprocessing.LabelBinarizer()\n",
        "    lb.fit(y_test)\n",
        "    y_test = lb.transform(y_test)\n",
        "    pred = lb.transform(pred)\n",
        "    return roc_auc_score(y_test, pred, average=average)"
      ],
      "metadata": {
        "id": "ytJIXz5k8sLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train data')\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print('='*20)\n",
        "print('Test data')\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)\n",
        "print('='*20)\n",
        "\n",
        "# Measure accuracy\n",
        "print('Predicting on the test data:')\n",
        "start = dt.datetime.now()\n",
        "escore = model.evaluate(X_test, Y_test, batch_size=128)\n",
        "pred = model.predict(X_test)\n",
        "pred = np.argmax(pred, axis=1)\n",
        "y_eval = np.argmax(Y_test, axis=1)\n",
        "\n",
        "# Compute various evaluation metrics\n",
        "vscore = accuracy_score(y_eval, pred)\n",
        "rscore = recall_score(y_eval, pred, average='weighted')\n",
        "ascore = precision_score(y_eval, pred, average='weighted')\n",
        "f1score = f1_score(y_eval, pred, average='weighted')  # F1 = 2 * (precision * recall) / (precision + recall)\n",
        "accuracy = multiclass_roc_auc_score(y_eval, pred)\n",
        "\n",
        "# Adding MCC\n",
        "mcc = matthews_corrcoef(y_eval, pred)\n",
        "\n",
        "print('Completed')\n",
        "print('Time taken:', dt.datetime.now() - start)\n",
        "print('='*50)\n",
        "print(\"Validation score: {}\".format(vscore))\n",
        "print('='*50)\n",
        "print(\"Evaluation score: {}\".format(escore))\n",
        "print('='*50)\n",
        "print(\"Recall score: {}\".format(rscore))\n",
        "print('='*50)\n",
        "print(\"Precision score: {}\".format(ascore))\n",
        "print('='*50)\n",
        "print(\"F1 score: {}\".format(f1score))\n",
        "print('='*50)\n",
        "print(\"MCC score: {}\".format(mcc))  # Printing MCC score\n",
        "print('='*50)\n"
      ],
      "metadata": {
        "id": "V-ab1Y1U8vGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = his.history\n",
        "history.keys()"
      ],
      "metadata": {
        "id": "Bvzk9bEs89Gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(history['loss']) + 1)\n",
        "acc = history['accuracy']\n",
        "loss = history['loss']\n",
        "val_acc = history['val_accuracy']\n",
        "val_loss = history['val_loss']\n",
        "\n",
        "# visualize training and val accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Accuracy (CNN)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(epochs, val_acc, label='val_acc')\n",
        "plt.plot(epochs, acc, label='accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# visualize train and val loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Loss(CNN)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(epochs, val_loss, label='val_loss', color='r')\n",
        "plt.plot(epochs, loss, label='loss', color='g')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "5NUnSj0i9BBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix_func(y_eval, pred)"
      ],
      "metadata": {
        "id": "GppJ-jxF9Q6Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}